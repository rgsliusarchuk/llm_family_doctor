{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7caef85",
   "metadata": {},
   "source": [
    "# üîß Data Preparation & Index Testing Notebook\n",
    "\n",
    "This notebook helps you:\n",
    "1. Set up the environment for both local and Google Colab\n",
    "2. Ingest PDF protocols into markdown\n",
    "3. Build a FAISS vector index\n",
    "4. Test the index with various queries\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Setup\n",
    "Run this cell first to configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a04b9dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Installing in local environment...\n",
      "üìÅ Looking for requirements.txt at: /Users/rsliusarchuk/www/genai/llm_family_doctor/requirements.txt\n",
      "‚úÖ Dependencies installed successfully\n"
     ]
    }
   ],
   "source": [
    "# üîß Install dependencies\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "def install_requirements():\n",
    "    # Get the path to requirements.txt (parent directory of notebooks/)\n",
    "    project_root = Path.cwd().parent\n",
    "    requirements_path = project_root / 'requirements.txt'\n",
    "    \n",
    "    try:\n",
    "        import google.colab\n",
    "        print(\"üîÑ Installing in Google Colab environment...\")\n",
    "        # Colab: use magic with correct path\n",
    "        get_ipython().system(f'pip install -q -r {requirements_path}')\n",
    "    except ImportError:\n",
    "        print(\"üîÑ Installing in local environment...\")\n",
    "        print(f\"üìÅ Looking for requirements.txt at: {requirements_path}\")\n",
    "        if not requirements_path.exists():\n",
    "            print(f\"‚ùå Requirements file not found at {requirements_path}\")\n",
    "            # Try alternative paths\n",
    "            alt_paths = [Path.cwd() / 'requirements.txt', Path.cwd().parent / 'requirements.txt']\n",
    "            for alt_path in alt_paths:\n",
    "                if alt_path.exists():\n",
    "                    print(f\"‚úÖ Found requirements.txt at: {alt_path}\")\n",
    "                    requirements_path = alt_path\n",
    "                    break\n",
    "            else:\n",
    "                print(\"‚ùå Could not find requirements.txt in any expected location\")\n",
    "                return\n",
    "        \n",
    "        result = subprocess.run([sys.executable, '-m', 'pip', 'install', '-r', str(requirements_path)], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(\"‚úÖ Dependencies installed successfully\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Installation issues: {result.stderr}\")\n",
    "\n",
    "install_requirements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2aec9731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ  Environment configured\n",
      "{'INDEX_PATH': '/Users/rsliusarchuk/www/genai/llm_family_doctor/data/faiss_index',\n",
      " 'MAP_PATH': '/Users/rsliusarchuk/www/genai/llm_family_doctor/data/doc_map.pkl',\n",
      " 'MODEL_ID': 'intfloat/multilingual-e5-base',\n",
      " 'OPENAI_API_KEY': 'ENTER_YOUR_OPENAI_API_KEY',\n",
      " 'OPENAI_MODEL': 'gpt-4.1-nano'}\n"
     ]
    }
   ],
   "source": [
    "# üîß Config | run once per session\n",
    "import os, pathlib, textwrap, pprint\n",
    "\n",
    "# Get project root\n",
    "project_root = Path.cwd().parent\n",
    "\n",
    "os.environ[\"MODEL_ID\"]   = \"intfloat/multilingual-e5-base\"\n",
    "os.environ[\"INDEX_PATH\"] = str(project_root / \"data/faiss_index\")\n",
    "os.environ[\"MAP_PATH\"]   = str(project_root / \"data/doc_map.pkl\")\n",
    "\n",
    "# Replace this with your actual OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"ENTER_YOUR_OPENAI_API_KEY\"\n",
    "os.environ[\"OPENAI_MODEL\"] = \"gpt-4.1-nano\"\n",
    "\n",
    "env_path = project_root / \".env\"\n",
    "env_path.write_text(textwrap.dedent(f\"\"\"\n",
    "    MODEL_ID={os.environ['MODEL_ID']}\n",
    "    INDEX_PATH={os.environ['INDEX_PATH']}\n",
    "    MAP_PATH={os.environ['MAP_PATH']}\n",
    "    OPENAI_API_KEY={os.environ['OPENAI_API_KEY']}\n",
    "    OPENAI_MODEL={os.environ['OPENAI_MODEL']}\n",
    "\"\"\").strip())\n",
    "\n",
    "print(\"‚úÖ  Environment configured\")\n",
    "pprint.pprint({k: os.environ[k] for k in (\"MODEL_ID\", \"INDEX_PATH\", \"MAP_PATH\", \"OPENAI_API_KEY\", \"OPENAI_MODEL\")})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8cebbd",
   "metadata": {},
   "source": [
    "## üìÅ Check Data Structure\n",
    "Let's verify our data directories exist and see what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e56da3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ RAW_PDFS: /Users/rsliusarchuk/www/genai/llm_family_doctor/data/raw_pdfs\n",
      "   ‚úÖ Exists with 37 items\n",
      "   - 3803.pdf\n",
      "   - 2937.pdf\n",
      "   - 3354.pdf\n",
      "   - 3744.pdf\n",
      "   - 3023.pdf\n",
      "   ... and 32 more\n",
      "\n",
      "üìÅ PROTOCOLS: /Users/rsliusarchuk/www/genai/llm_family_doctor/data/protocols\n",
      "   ‚úÖ Exists with 35 items\n",
      "   - nastanova_00172_nudota_ta_blyuvannya.md\n",
      "   - nastanova_00047_vaktsynatsiya.md\n",
      "   - nastanova_00745_zapamorochennya.md\n",
      "   - nastanova_01026_enterovirusni_infektsiyi.md\n",
      "   - nastanova_00015_hryp.md\n",
      "   ... and 30 more\n",
      "\n",
      "üìÅ INDEX: /Users/rsliusarchuk/www/genai/llm_family_doctor/data/faiss_index\n",
      "   ‚úÖ File exists\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def check_data_structure():\n",
    "    # Get the project root (parent of notebooks directory)\n",
    "    project_root = Path.cwd().parent\n",
    "    directories = {\n",
    "        'raw_pdfs': project_root / 'data/raw_pdfs',\n",
    "        'protocols': project_root / 'data/protocols',\n",
    "        'index': project_root / 'data/faiss_index',\n",
    "    }\n",
    "    for name, path in directories.items():\n",
    "        print(f'üìÅ {name.upper()}: {path}')\n",
    "        if path.exists():\n",
    "            if path.is_dir():\n",
    "                files = list(path.glob('*'))\n",
    "                print(f'   ‚úÖ Exists with {len(files)} items')\n",
    "                for file in files[:5]:\n",
    "                    print(f'   - {file.name}')\n",
    "                if len(files) > 5:\n",
    "                    print(f'   ... and {len(files) - 5} more')\n",
    "            else:\n",
    "                print('   ‚úÖ File exists')\n",
    "        else:\n",
    "            print('   ‚ùå Does not exist')\n",
    "        print()\n",
    "\n",
    "check_data_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c9e71c",
   "metadata": {},
   "source": [
    "## üìÑ Ingest PDF Protocols\n",
    "Convert PDF files to markdown format for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d26986a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Found 70 PDF files to process\n",
      "‚úÖ PDF ingestion completed successfully\n",
      "Done.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def run_ingest():\n",
    "    # Get the project root (parent of notebooks directory)\n",
    "    project_root = Path.cwd().parent\n",
    "    pdf_dir = project_root / 'data/raw_pdfs'\n",
    "    if not pdf_dir.exists():\n",
    "        print('‚ö†Ô∏è  No data/raw_pdfs directory found. Creating it...')\n",
    "        pdf_dir.mkdir(parents=True, exist_ok=True)\n",
    "        print('üìù Please add PDF files to data/raw_pdfs/ and run this cell again.')\n",
    "        return False\n",
    "    pdf_files = list(pdf_dir.glob('*.pdf')) + list(pdf_dir.glob('**/*.pdf'))\n",
    "    if not pdf_files:\n",
    "        print('‚ö†Ô∏è  No PDF files found in data/raw_pdfs/')\n",
    "        print('üìù Please add PDF files and run this cell again.')\n",
    "        return False\n",
    "    print(f'üìÑ Found {len(pdf_files)} PDF files to process')\n",
    "    \n",
    "    # Change to project root directory before running the script\n",
    "    original_cwd = Path.cwd()\n",
    "    try:\n",
    "        os.chdir(project_root)\n",
    "        result = subprocess.run([sys.executable, 'scripts/ingest_protocol.py', '--dir', 'data/raw_pdfs', '--recursive'], capture_output=True, text=True)\n",
    "    finally:\n",
    "        os.chdir(original_cwd)  # Always restore original directory\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print('‚úÖ PDF ingestion completed successfully')\n",
    "        print(result.stdout)\n",
    "        return True\n",
    "    else:\n",
    "        print(f'‚ùå Ingestion failed: {result.stderr}')\n",
    "        return False\n",
    "\n",
    "run_ingest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dab0126",
   "metadata": {},
   "source": [
    "## üîç Build Vector Index\n",
    "Create FAISS index from the markdown protocols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2136d5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Found 34 markdown files to index\n",
      "‚úÖ Index building completed successfully\n",
      "üîπ Loading intfloat/multilingual-e5-base ‚Ä¶\n",
      "üîπ Encoding 34 documents\n",
      "‚úÖ  Saved index ‚Üí /Users/rsliusarchuk/www/genai/llm_family_doctor/data/faiss_index  (vectors: 34)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_build_index():\n",
    "    # Get the project root (parent of notebooks directory)\n",
    "    project_root = Path.cwd().parent\n",
    "    md_dir = project_root / 'data/protocols'\n",
    "    if not md_dir.exists():\n",
    "        print('‚ö†Ô∏è  No data/protocols directory found.')\n",
    "        print('üìù Please run the ingestion step first.')\n",
    "        return False\n",
    "    md_files = list(md_dir.glob('*.md'))\n",
    "    if not md_files:\n",
    "        print('‚ö†Ô∏è  No markdown files found in data/protocols/')\n",
    "        print('üìù Please run the ingestion step first.')\n",
    "        return False\n",
    "    print(f'üìÑ Found {len(md_files)} markdown files to index')\n",
    "    \n",
    "    # Change to project root directory before running the script\n",
    "    original_cwd = Path.cwd()\n",
    "    try:\n",
    "        os.chdir(project_root)\n",
    "        result = subprocess.run([sys.executable, 'src/indexing/build_index.py', '--hf-model', os.environ['MODEL_ID']], capture_output=True, text=True)\n",
    "    finally:\n",
    "        os.chdir(original_cwd)  # Always restore original directory\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print('‚úÖ Index building completed successfully')\n",
    "        print(result.stdout)\n",
    "        return True\n",
    "    else:\n",
    "        print(f'‚ùå Index building failed: {result.stderr}')\n",
    "        return False\n",
    "\n",
    "run_build_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601e01b9",
   "metadata": {},
   "source": [
    "## üß™ Test the Index Database\n",
    "Load the index and test it with various queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df28ebcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded: intfloat/multilingual-e5-base\n",
      "‚úÖ Index loaded: 34 documents, 768 dimensions\n",
      "‚úÖ Document map loaded: 34 entries\n",
      "‚úÖ Index loaded successfully! Ready for testing.\n"
     ]
    }
   ],
   "source": [
    "import faiss, pickle, numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def load_index():\n",
    "    try:\n",
    "        # Get the project root (parent of notebooks directory)\n",
    "        project_root = Path.cwd().parent\n",
    "        index_path = project_root / 'data/faiss_index'\n",
    "        map_path = project_root / 'data/doc_map.pkl'\n",
    "        \n",
    "        if not index_path.exists():\n",
    "            print(f'‚ùå Index file not found: {index_path}')\n",
    "            return None, None, None\n",
    "        if not map_path.exists():\n",
    "            print(f'‚ùå Map file not found: {map_path}')\n",
    "            return None, None, None\n",
    "        model = SentenceTransformer(os.environ['MODEL_ID'])\n",
    "        print(f'‚úÖ Model loaded: {os.environ[\"MODEL_ID\"]}')\n",
    "        index = faiss.read_index(str(index_path))\n",
    "        print(f'‚úÖ Index loaded: {index.ntotal} documents, {index.d} dimensions')\n",
    "        with open(map_path, 'rb') as f:\n",
    "            doc_map = pickle.load(f)\n",
    "        print(f'‚úÖ Document map loaded: {len(doc_map)} entries')\n",
    "        if index.ntotal != len(doc_map):\n",
    "            print(f'‚ö†Ô∏è  Warning: Index has {index.ntotal} docs, map has {len(doc_map)} entries')\n",
    "        return model, index, doc_map\n",
    "    except Exception as e:\n",
    "        print(f'‚ùå Error loading index: {e}')\n",
    "        import traceback; traceback.print_exc()\n",
    "        return None, None, None\n",
    "\n",
    "def search_documents(model, index, doc_map, query, k=3):\n",
    "    try:\n",
    "        vec = model.encode(query, normalize_embeddings=True).astype('float32')[None]\n",
    "        D, I = index.search(vec, k)\n",
    "        print(f'üîç Query: \"{query}\"')\n",
    "        print(f'üìä Found {len(I[0])} results:')\n",
    "        for rank, (idx, score) in enumerate(zip(I[0], D[0]), 1):\n",
    "            if 0 <= idx < len(doc_map):\n",
    "                content = doc_map[idx]\n",
    "                lines = content.split('\\n')\n",
    "                title = lines[0] if lines else 'No title'\n",
    "                preview = content[:200].replace('\\n', ' ').strip()\n",
    "                print(f'   {rank}. Similarity: {score:.3f}')\n",
    "                print(f'      üìÑ {title}')\n",
    "                print(f'      üìù {preview}...')\n",
    "            else:\n",
    "                print(f'   ‚ùå Index {idx} out of range!')\n",
    "            print()\n",
    "    except Exception as e:\n",
    "        print(f'‚ùå Error during search: {e}')\n",
    "\n",
    "model, index, doc_map = load_index()\n",
    "if model and index and doc_map:\n",
    "    print('‚úÖ Index loaded successfully! Ready for testing.')\n",
    "else:\n",
    "    print('‚ùå Failed to load index. Please run the previous cells first.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c537164",
   "metadata": {},
   "source": [
    "## üîç Test Queries\n",
    "Try different queries to test the index functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17cb6a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing index with various medical queries...\n",
      "\n",
      "============================================================\n",
      "üîç Query: \"–≥–æ–ª–æ–≤–Ω–∏–π –±—ñ–ª—å –≤ —Å–∫—Ä–æ–Ω–µ–≤—ñ–π –¥—ñ–ª—è–Ω—Ü—ñ\"\n",
      "üìä Found 2 results:\n",
      "   1. Similarity: 0.871\n",
      "      üìÑ # –ù–∞—Å—Ç–∞–Ω–æ–≤–∞ 00791. –ì–æ–ª–æ–≤–Ω–∏–π –±—ñ–ª—å –Ω–∞–ø—Ä—É–≥–∏\n",
      "      üìù # –ù–∞—Å—Ç–∞–Ω–æ–≤–∞ 00791. –ì–æ–ª–æ–≤–Ω–∏–π –±—ñ–ª—å –Ω–∞–ø—Ä—É–≥–∏  –ù–∞—Å—Ç–∞–Ω–æ–≤–∞ 00791. –ì–æ–ª–æ–≤–Ω–∏–π –±—ñ–ª—å –Ω–∞–ø—Ä—É–≥–∏ –î–ª—è –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤ —á–∏ —ñ–Ω—à–æ–≥–æ –∑–≤–æ—Ä–æ—Ç–Ω–æ–≥–æ –∑–≤'—è–∑–∫—É –∑–∞–ø–æ–≤–Ω—ñ—Ç—å —Ñ–æ—Ä–º—É: —Ñ–æ—Ä–º–∞ –∑–≤–æ—Ä–æ—Ç–Ω–æ–≥–æ –∑–≤'—è–∑–∫—É —â–æ–¥–æ —Ü—ñ—î—ó –≤–µ—Ä—Å—ñ—ó –Ω–∞—Å—Ç–∞–Ω–æ–≤–∏ –í–µ—Ä—Å—ñ—è...\n",
      "\n",
      "   2. Similarity: 0.863\n",
      "      üìÑ # –ù–∞—Å—Ç–∞–Ω–æ–≤–∞ 00743. –ì–æ–ª–æ–≤–Ω–∏–π –±—ñ–ª—å\n",
      "      üìù # –ù–∞—Å—Ç–∞–Ω–æ–≤–∞ 00743. –ì–æ–ª–æ–≤–Ω–∏–π –±—ñ–ª—å  –ù–∞—Å—Ç–∞–Ω–æ–≤–∞ 00743. –ì–æ–ª–æ–≤–Ω–∏–π –±—ñ–ª—å –î–ª—è –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤ —á–∏ —ñ–Ω—à–æ–≥–æ –∑–≤–æ—Ä–æ—Ç–Ω–æ–≥–æ –∑–≤'—è–∑–∫—É –∑–∞–ø–æ–≤–Ω—ñ—Ç—å —Ñ–æ—Ä–º—É: —Ñ–æ—Ä–º–∞ –∑–≤–æ—Ä–æ—Ç–Ω–æ–≥–æ –∑–≤'—è–∑–∫—É —â–æ–¥–æ —Ü—ñ—î—ó –≤–µ—Ä—Å—ñ—ó –Ω–∞—Å—Ç–∞–Ω–æ–≤–∏ –í–µ—Ä—Å—ñ—è —Ü—å–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç—É...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if model and index and doc_map:\n",
    "    test_queries = [\n",
    "        '–≥–æ–ª–æ–≤–Ω–∏–π –±—ñ–ª—å –≤ —Å–∫—Ä–æ–Ω–µ–≤—ñ–π –¥—ñ–ª—è–Ω—Ü—ñ'\n",
    "    ]\n",
    "    print('üß™ Testing index with various medical queries...\\n')\n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f'{\"=\"*60}')\n",
    "        search_documents(model, index, doc_map, query, k=2)\n",
    "        if i < len(test_queries):\n",
    "            print()\n",
    "else:\n",
    "    print('‚ùå Index not loaded. Please run the previous cells first.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd209cc",
   "metadata": {},
   "source": [
    "## üéØ Interactive Search\n",
    "Test your own queries here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd5f05ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Test your own query:\n",
      "Example: search_documents(model, index, doc_map, 'your query here', k=3)\n"
     ]
    }
   ],
   "source": [
    "def interactive_search():\n",
    "    if not (model and index and doc_map):\n",
    "        print('‚ùå Index not loaded. Please run the previous cells first.')\n",
    "        return\n",
    "    print('üéØ Interactive Search Mode')\n",
    "    print('Enter your medical query (or \\'quit\\' to exit):')\n",
    "    while True:\n",
    "        try:\n",
    "            query = input('\\nüîç Query: ').strip()\n",
    "            if query.lower() in ['quit', 'exit', 'q']:\n",
    "                print('üëã Goodbye!')\n",
    "                break\n",
    "            if not query:\n",
    "                print('‚ö†Ô∏è  Please enter a query.')\n",
    "                continue\n",
    "            try:\n",
    "                k = int(input('üìä Number of results (default 3): ') or '3')\n",
    "                k = max(1, min(k, 10))\n",
    "            except ValueError:\n",
    "                k = 3\n",
    "            print()\n",
    "            search_documents(model, index, doc_map, query, k)\n",
    "        except KeyboardInterrupt:\n",
    "            print('\\nüëã Goodbye!')\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f'‚ùå Error: {e}')\n",
    "# Uncomment to enable interactive search in notebook\n",
    "# interactive_search()\n",
    "if model and index and doc_map:\n",
    "    print('üéØ Test your own query:')\n",
    "    print(\"Example: search_documents(model, index, doc_map, 'your query here', k=3)\")\n",
    "else:\n",
    "    print('‚ùå Index not loaded. Please run the previous cells first.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027e6862",
   "metadata": {},
   "source": [
    "## üìä Index Statistics\n",
    "Get detailed information about the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce23e2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Index Statistics\n",
      "==================================================\n",
      "üìÑ Total documents: 34\n",
      "üî¢ Vector dimensions: 768\n",
      "üóÇÔ∏è  Document map entries: 34\n",
      "ü§ñ Model: intfloat/multilingual-e5-base\n",
      "üíæ Index file: /Users/rsliusarchuk/www/genai/llm_family_doctor/data/faiss_index\n",
      "üó∫Ô∏è  Map file: /Users/rsliusarchuk/www/genai/llm_family_doctor/data/doc_map.pkl\n",
      "\n",
      "üìè Document Length Statistics:\n",
      "   Min: 2,000 characters\n",
      "   Max: 2,000 characters\n",
      "   Avg: 2,000 characters\n",
      "\n",
      "üìã Sample Document Titles:\n",
      "   1. # –ù–∞—Å—Ç–∞–Ω–æ–≤–∞ 00006. –Ü–Ω—Ñ–µ–∫—Ü—ñ—ó –¥–∏—Ö–∞–ª—å–Ω–∏—Ö —à–ª—è—Ö—ñ–≤ —É –¥–æ—Ä–æ—Å–ª–∏—Ö...\n",
      "   2. # –ù–∞—Å—Ç–∞–Ω–æ–≤–∞ 00007. –§–∞—Ä–∏–Ω–≥—ñ—Ç —ñ —Ç–æ–Ω–∑–∏–ª—ñ—Ç...\n",
      "   3. # –ù–∞—Å—Ç–∞–Ω–æ–≤–∞ 00015. –ì—Ä–∏–ø...\n",
      "   4. # –ù–∞—Å—Ç–∞–Ω–æ–≤–∞ 00047. –í–∞–∫—Ü–∏–Ω–∞—Ü—ñ—è...\n",
      "   5. # –ù–∞—Å—Ç–∞–Ω–æ–≤–∞ 00099. –ù–∞–±—Ä—è–∫ –Ω—ñ–≥...\n",
      "   ... and 29 more documents\n"
     ]
    }
   ],
   "source": [
    "def show_index_stats():\n",
    "    if not (model and index and doc_map):\n",
    "        print('‚ùå Index not loaded. Please run the previous cells first.')\n",
    "        return\n",
    "    print('üìä Index Statistics')\n",
    "    print('=' * 50)\n",
    "    print(f'üìÑ Total documents: {index.ntotal}')\n",
    "    print(f'üî¢ Vector dimensions: {index.d}')\n",
    "    print(f'üóÇÔ∏è  Document map entries: {len(doc_map)}')\n",
    "    print(f'ü§ñ Model: {os.environ[\"MODEL_ID\"]}')\n",
    "    print(f'üíæ Index file: {os.environ[\"INDEX_PATH\"]}')\n",
    "    print(f'üó∫Ô∏è  Map file: {os.environ[\"MAP_PATH\"]}')\n",
    "    doc_lengths = [len(doc) for doc in doc_map]\n",
    "    print(f'\\nüìè Document Length Statistics:')\n",
    "    print(f'   Min: {min(doc_lengths):,} characters')\n",
    "    print(f'   Max: {max(doc_lengths):,} characters')\n",
    "    print(f'   Avg: {sum(doc_lengths)/len(doc_lengths):,.0f} characters')\n",
    "    print(f'\\nüìã Sample Document Titles:')\n",
    "    for i, doc in enumerate(doc_map[:5]):\n",
    "        title = doc.split('\\n')[0] if doc else 'No title'\n",
    "        print(f'   {i+1}. {title[:60]}...')\n",
    "    if len(doc_map) > 5:\n",
    "        print(f'   ... and {len(doc_map) - 5} more documents')\n",
    "show_index_stats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
